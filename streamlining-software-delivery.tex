\documentclass[english]{tktltiki2}

% -- Packages --

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% -- Language --

\usepackage[fixlanguage]{babelbib}

% Add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}

% -- tktltiki2 options --

\title{Advances in Streamlining Software Delivery on the Web and its Relations to Embedded Systems}
\author{Kasper Hirvikoski}
\date{\today}
\level{Master’s thesis}
\abstract{Abstract.}

\keywords{keyword}
\classification{}

\begin{document}

% -- Front matter --

\frontmatter

\maketitle
\setcounter{page}{2}
\makeabstract

\tableofcontents

% -- Main matter --

\mainmatter

% -- Introduction --

\section{Introduction}

Software delivery on the web has over the years evolved into a rather established process. A software is developed iteratively through multiple phases, which ensure the user’s requirements and the quality of the product or service. These phases form what is called the deployment pipeline~\cite{Fow06, HF11, Fow13a, Fow13b}.

The deployment pipeline nowadays usually consists of at least three stages: development, staging and production. Organisations alter these depending on their size and needs. Using modern iterative and incremental processes, a software is developed feature-by-feature by iterating through these steps. Development starts in the development stage where developers build the feature requested by the customer or user. The feature is then tested in the staging phase, which represents the production setting. When the feature has been validated, it is then deployed to production. If necessary, each stage can be repeated until the feature is accepted. The steps are short and features are deployed frequently — in some cases even multiple times a day~\cite{OR11, Sny13, Rub14}.

Software engineering consists of various different processes and practises for ensuring the quality of the product or service — nowadays more or less based on Agile and Lean ideologies and practises~\cite{Ono88, BBB01a, Fow05, Mon12}. At the low level, developers use source code management to keep track of changes to the software and to collaborate with other team members. To reinforce that the features work as intended, developers write automated test cases. Teams can also use more social methods — such as reviewing each other’s code — to validate the implementations. These practises form the basis for Continuous Integration and Continuous Deployment~\cite{Fow06, HF11, Fow13a, Fow13b}. Software changes are frequently integrated, tested and deployed — automatically in each stage. The first two form Continuous Integration and the latter Continuous Deployment. If any stage fails, the process starts from the beginning.

The web enables the use of the deployment pipeline and its practises in an unprecedented way~\cite{KLS09}. Due to the distributed nature of the web, software can be deployed as needed and the user always sees the newest version without the need of any interaction. This eases the use of many cutting-edge methods~\cite{KLS09, FGM14}. Deploying software as needed has allowed developers to experiment with different implementations of a feature. These changes can target anything from a more optimised algorithm to something more user-faced, such as improvements to the user experience of a product~\cite{KLS09}. These practises have started to formalise as Continuous Experimentation~\cite{FGM14}.

Not all software can be developed easily this way. Many embedded systems, which have a dedicated function within a larger mechanical or electrical system, require hardware to accompany the software. This presents a variety of challenges to overcome. Hardware can require thorough planning and iterating can take time. Contexts such as cross-platform support, robotics, aerospace and other embedded systems pose interesting cases. Many of these contexts can at a glance seem regarded as models for more traditional sequential software engineering processes with heavy planning, documentation and long development phases. However, even NASA’s earlier missions have iterated on the successes and failures of previous ones~\cite{LB03}. Even though it can be more difficult, software related to hardware can be build and tested iteratively~\cite{LB03}. New approaches such as prototyping and 3D-printing provide novel ways for even building hardware iteratively.

This raises an interesting research topic — \emph{presenting the advances in streamlining software delivery on the web and relating its practises and their advantages and challenges to embedded systems}. Using case studies to identify which Agile and Lean practises are used, how they could be improved and how new practises could be incorporated to these settings. Moreover, the aim is to identify which modern Continuous Integration, Delivery and Experimentation practises are used. Can we determine how they compare to the way the web is utilised as a platform?

The hypothesis is that there should be no reason why these practises could not be successfully used and cleverly adapted to hardware settings. My research method for this thesis was reviewing the current practises in literature and industry. I also conducted several semi-structured interviews with the industry working on leading embedded systems to get a view on if and how the deployment pipeline has changed the development of hardware related products.

This thesis is structured into seven chapters. Following the introduction, Chapter 2 outlines how software delivery has progressed from a structureless process following a code-and-fix mentality, to what is now considered the leading edge of iterative development. This sets the scene for understanding the rationality behind being adaptive to change and how the user is an essential part of the process. Chapter 3 describes how software delivery has embraced primarily a three staged pipeline for deploying new features to the user. Chapter 4 discusses how the web has provided an effective platform for the deployment pipeline by streamlining and automating many of the practises used by modern development. Chapter 5 delves into the challenges related to delivering software that is firmly linked to hardware. It deliberates about how the deployment pipeline could be integrated into these embedded systems. Chapter 6 presents the results from the interviews I collected from the field. The idea is to incite discussion — through the view of people working on embedded systems — about what is the current state of software delivery in these settings and how it could be improved. Finally Chapter 7 concludes this work by making conclusions about the gathered knowledge.

% -- Software Delivery --

\section{Software Delivery}

Software development has changed notably in the past few decades, nonetheless it is still a young field. Most software development can be seen as disordered chaos with a mentality of coding first and fixing later~\cite{Boe88, Fow05}. A software is built without much of an underlying plan and the design of the system is a result of many short term decisions. This can work well if the system is small, but as it grows, adding new features becomes easily too much to bear. Going back, it was not until 1968, when the term software engineering was introduced by the NATO Science Committee~\cite{NR69}. By that time, it was considered that software development had drifted into a crisis, where a wider gap was forming between the objectives and end-results of software projects. Additionally, it was getting increasingly difficult to plan the cost of development. A typical consequence was a long test phase after a system was considered “feature complete”~\cite{Fow05}. A collective effort was put in place to establish a more formalised method for software development — similar to traditional engineering such as building bridges. It was considered necessary that the foundation for delivering software should be more theoretical with laid principles and practises~\cite{NR69}. Software development had to be more predictable and efficient. By 1969, the term software engineering had become well-established~\cite{BR70}.

Software development processes began to form. One of the primary functions of software process models was determine the flow and order how software is developed in stages~\cite{Boe88}. Notably in 1970, Winston W. Royce published a paper that described a formal approach for sequentially developing a software based on previously used practises~\cite{Roy70}. It was only later named as the waterfall model~\cite{Boe88, LB03}. The process consists of multiple stages that should be carried after the previous has been reviewed and verified. See figure~\ref{figure:waterfall-model}. It begins by mapping the requirements for the entire software, then proceeding to designing the architecture, followed by implementing the plan, verifying the result is according to the set requirements, and finally maintaining the product~\cite{Roy70}. Each stage is planned and documented thoroughly. The concept being that as each step progresses, the design of the software is further detailed. However, contrary to what has been referred, Royce presented the model as a flawed, non-working model~\cite{Roy70}. If any of the stages fail, serious reconsideration of the plan or implementation might be necessary. Therefore sequentially following the stages would not produce what was intended and inevitably previous stages would need to be revisited~\cite{Roy70}. Royce however found the approach fundamentally sound and proposed that the method should be carried out twice~\cite{Roy70, Boe88}. It should start first by creating a prototype-like plan and only then proceed to execute it. Nevertheless, this was overlooked and the waterfall model became the dominant software development process for software standards in government and industry for the time-being~\cite{Boe88, LB03}. Royce has later been stated as a supporter for iterative approaches~\cite{LB03}.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/waterfall-model}

    \caption{Waterfall Model}
    \label{figure:waterfall-model}

    \vspace{1cm}

\end{figure}

Engineering methodologies, also called as plan-driven methods, are considered heavy. The waterfall model has been criticised as too controlled, managed and documentation-oriented~\cite{Boe88, LB03, Fow05}. They also have not been noted for being terribly successful~\cite{Fow05}. Royce defined software as done only when the documentation of it was adequate~\cite{Roy70}. It was declared that developers should prioritise in keeping the documentations up to date. More lightweight iterative processes were proposed as opponents for incremental software development in the later part of the nineteen hundreds~\cite{LB03}. In fact, early applications of iterative and incremental development dates as far back as the mid-1950s — with many names such as incremental, evolutionary, spiral and staged~\cite{Boe88, LB03, Fow05}. The methods sought in developing a useful compromise between no process and too much process~\cite{Fow05}. They also focused to be less document-oriented and in many ways more code-oriented. It was considered that the documentation for a project should be the code itself, not some external specification.

Fast-forward to 2001, when a group of software developers met to discuss new lightweight development principles. As the result of these discussions, a manifesto for Agile software development was published~\cite{BBB01a}. Four principles were proposed for Agile software development: \emph{focusing on individuals and interactions} over processes and tools, \emph{focusing on working software} over comprehensive documentation, \emph{focusing on customer collaboration} over contract negotiation and \emph{responding to change} over following a plan. The manifesto does not dismiss the value of the latter, but considers the former more valuable~\cite{BBB01a}. From thereon, iterative processes started to gain mainstream traction~\cite{LB03, Fow05}.

Software development was considered as an ongoing process, where a product should be build in small increments, iteratively going through the development stages. The notion was not to resist change. Most of the ideas were not new and had been successfully used already in the industry for a long time~\cite{Fow05}. An urge was revived to treat the ideas more seriously. Instead of planning, designing and implementing the whole software, the software should be build iteratively by repeating all of these steps in shorter more manageable parts. See figure~\ref{figure:iterative-development}. Hence, any issue or miss-communication could be discovered early and fixed accordingly.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/iterative-development}

    \caption{Iterative Development}
    \label{figure:iterative-development}

\end{figure}

\subsection{Adapting to Requirements}

The demands for software products are continuously shifting. It is not always obvious what the users want. In some cases, users do not know what they are looking for, until you show them what they need. It is hard to know what the value of a feature is before you see it in reality~\cite{Fow05}. Actuality allows the user to learn how a feature works. An average client has little knowledge on how software products work or how they are built. Therefor, it is exceedingly difficult for a client to map specifically what they require from a software product. Software development should be more people-oriented than process-oriented~\cite{Fow05}. This requires a different kind of relationship with the customer. What is notable, is that even Royce emphasised, although loosely, the value of customer commitment during development~\cite{Roy70}.

In most cases, rigorously planning a software beforehand will not work~\cite{LB03}. It is not uncommon that an idea will change quite a bit during its lifetime. The key problem that plan-driven  methods face is the separation of design~\cite{LB03, Fow05}. The concept was similar to traditional engineering: engineers would build a precise plan which would then be followed by a different set of people. In such, architects would first design the bridge and then a construction company would build it. Designing, which involves creative and more expensive individuals, is far more difficult and less predictable than construction~\cite{Fow05}. Construction on the other hand, although more labour intensive, is considered more predictable and straightforward after a plan has been completed. The premise was that by following this methodology in software engineering, we could reasonably predict the time and cost of software construction. When Royce first defined the waterfall model, he stated that the documentation of a software is both its specification and design~\cite{Roy70}. Without documentation, there would be no design and no communication.

Still, no one has found a solid way of designing software in a manner that the plans can be verified before construction~\cite{Fow05}. A design can look good on paper, but be seriously flawed when you actually program it. When building a bridge, the cost of the design is fractional to the cost of construction~\cite{Fow05}. In software, the time spent coding is fractional to the time spent designing. Essentially, coding is designing. Coding requires creative and talented people. People are the most important factor in software development. Developers should be in control of all the technical decisions. There are serious flaws in separating different tasks to different “specialists”, but this is how engineering was regarded as~\cite{Roy70}. It is still quite common that a developer writing the code and a tester writing the tests, are not the same person. Even though studies have indicated that developers writing the functionality tend to write more rigorous test cases~\cite{MND09}. The metaphor for traditional engineering is in practise flawed~\cite{Fow05}. A grave report from late 1990s found out that 75\% of projects for the US Department of Defence failed or where never actually used~\cite{LB03}. Only a slim 2\% were used without the need for substantial changes. Other reports have also indicated that one of the top reasons for project failures is related to waterfall practises~\cite{LB03}.

Andy Whitlock, a product strategist, drew a fitting mental picture about changes~\cite{Whi14}. You see the road ahead as a clear and straight path to an objective you have set. What you do not always realise, is that the path will have its twists and turns along the way. What you can really only do, is to plan to a certain point ahead. The rest of your path will be a gloomy fog in the distance. You need to be ready to make difficult choices along the way. Agile development tries to create a framework, where processes and practises can take these requirements into consideration. Even to the point of changing the process itself~\cite{Fow05}.

Prominently, being “agile” means effectively responding to change and not resisting it. After all software is supposed to be “soft”~\cite{Fow05}. These course corrections are rapid and adaptive. The highest priority is to satisfy the customer trough continuously delivering valuable software from early on~\cite{BBB01b}. Software should be delivered frequently in short increments. These increments, also referred as iterations in Agile development, should take no more than a couple of weeks to a couple of months — the shorter the better~\cite{Fow05}. Each iteration a working software is delivered with a subset of the required features. These features should be as carefully tested as a final delivery. Throughout the project, teams respond to change by having effective communication among all stakeholders for the product daily. The best means for conveying information is face-to-face conversation~\cite{BBB01b}. At every iteration the customer has control over the process by getting a look on the progress and then altering the direction as needed. Continuous feedback has been attributed as a key factor for success~\cite{DD08}.

A stakeholder represents the views for the users or clients. By taking the stakeholders as part of the team, developers can react when something is not working as intended. Customer reviews and acceptance where already noted in the spiral model, which dates as early as the 1980s~\cite{Boe88}. Studies show that developers see the ongoing presence of stakeholders helpful for development~\cite{DD08}. An Agile process is driven by the customers descriptions of what is required~\cite{BBB01b}. These requirements may be short-lived and that must be kept in focus. Changes are unavoidable~\cite{Fow05}. Users’ desires evolve and this must be harnessed to the customer’s competitive advantage~\cite{BBB01b, Fow05}. Even if deciding a stable set of requirements would be possible, outside forces are changing the value of features too rapidly~\cite{Fow05}. It is not uncommon for requirements to change even late in development. If you cannot get fixed requirements, you cannot get a predictable plan. This is what makes plan-driven development inefficient. Royce stated that required design changes can be so disruptive that the software requirements upon which the design is based and which provide the rationale for everything can be breached~\cite{Roy70}. Even so predictability is highly desirable~\cite{Fow05}. It is an essential force in what makes a model work. Adaptivity is about making unpredictability predictable. This enables risk-control for the project.

One key premiss for Agile development is to reduce the burden of the process. Working software is the primary measure of progress~\cite{BBB01b}. A process should not hinder the work of a team — on the contrary it should permit the team to function to its full extent. By organising the team to be in control of the process, the framework facilitates rapid and incremental delivery of software. Still, no process will make up the skill of the individuals working on the project~\cite{Boe88, Fow05}. Projects should be based on motivated individuals~\cite{BBB01b}. Motivation is maintained by creating a constructive environment and giving the necessary support when needed. Trusting the team is of the utmost importance~\cite{BBB01b}. Morale affects the productivity of people~\cite{Fow05}.

One of the weaknesses of adaptability is that in its essence it implies that the usual notion of fixed-priced software development does not work~\cite{Fow05}. Instead completely new approaches have to be used. You cannot fix scope, time and price in the same way as plan-driven methods have tried. The usual agile approach is to fix time and price and allow the scope to vary in a predetermined manner. Value is not only created by building software on-time and on-cost, but by building software that is valuable to the customer.

\subsection{Ensuring Quality}

Assuring quality is not an easy task. Applying measurements to software development is demanding. Something as simple as productivity is exceedingly hard to quantify. ISO 9000 -standard defines quality as the extent of how well the characteristics of a product or service fulfil all of the requirements, the needs and expectations, set by the stakeholders~\cite{ISO9000}. IEEE defines software quality as the degree to which a system, component or process meets the specified requirements as well as the customer’s and user’s needs or expectations~\cite{IEEE1074}. Both definitions focus strongly on fulfilling the user’s needs.

Software development is challenging. Users perceive quality as working software, but most of all emphasising good technical design and implementation makes the development process easier. People, time and money are limiting factors for ensuring quality. Strict deadlines and scarce resources have direct effects. Furthermore, human factors play a considerable role~\cite{DD08}. Several empirical studies reinforce the significance of Agile development processes and practises as improving quality in software~\cite{DD08, SS10}. Evidently being “agile” should in the long term make development more predictable and eventually lead to shorter development times and minimised costs~\cite{DD08}. This provides an environment for being adaptive.

In addition to focusing on satisfying the customers needs, Agile development promotes continuous attention on technical excellence and good design practises~\cite{BBB01b}. Even so, this should not be accomplished by hindering simplicity. Simplicity maximises the amount of work that can be accomplished. The Agile Manifesto states that the best architectures, requirements and designs emerge from self-organising teams~\cite{BBB01b}. After regular intervals, the team members reflect on how they have performed and how they can become more effective. This is how the team can then tune and adjust its behaviour appropriately.

The practises also have their critics. One of the biggest criticism is that there is little scientific support for many of the claims made by the agile community~\cite{DD08}. Agile development has also been critiqued for a lack of focus on the architectures and design behind software. Practises are also rarely applicable by the book and therefor they are rarely used as such. Additionally, Agile development has a strong focus on small teams and so many have struggled in adapting them to larger environments. It is no surprise that it takes time and effort to introduce the methods properly~\cite{DD08}. In most cases, once you get past the first obstacles many of these hurdles come less critical.

\subsection{Processes and Practises}

Processes and practises assist the development process. They create the framework and guidelines within a team can develop a suitable environment to deliver software~\cite{Kni07}. The process is part of the design~\cite{Fow05}. They also help to maintain quality. Agile development has become well known and organisations are interested in adopting the methods~\cite{DD08}.

At the low level, developers use source code management to keep track of changes to the software and to collaborate with other team members. Source code management enables multiple developers to work on a single project, while also creating a history for the entire project. When a problem arises, developers can go back in time to look at the source code at any given point in time. To ensure features work as intended, developers use automated test cases to verify expected behaviour. There is a clear correlation between higher test coverage resulting in fewer errors in software~\cite{MND09}. Tested code has a better change of detecting errors than untested code. Teams can also use more social methods — such as reviewing each other’s code — to validate the implementations. Pair programming, coding dojos and hackathons provide tools for improving skills and solving complex problems together~\cite{DD08, HHL13}.

Most iterative development processes vary by iteration length and how iterations are time-boxed~\cite{LB03}. Agile development only provides a framework for software delivery. It does not specify concretely how development should be organised. Development methods focus on how you should develop. Most notably, Scrum and Extreme Programming have created a structure for Agile development~\cite{LB03, Fow05, SS10}. Scrum provides a framework for managing development. It focuses on how development should be planned, managed and scheduled. It does not provide any strict practises, instead it gives guidelines for how customer requirements should be discovered, prioritised, and how the development of these features is split into iterations.

Scrum has been strengthened with ideas and practises such as simple design, small releases, coding standards, test-driven development, refactoring, pair programming, collective ownership of the code, utilising a on-site customer and continuous integration~\cite{DD08}. These are defined in Extreme Programming. Continuous Integration aims at creating a process where developers integrate new features in small chunks and as ofter as possible into the software. In test-driven development, features are developed by writing the expectations for a feature as tests before actually implementing the code. When possible, existing code should also always be refactored to be better. In pair programming, developers develop features in pairs.

Extreme Programming practises have been more widely studied than Scrum and Lean~\cite{DD08}. Most of the practises have been regarded as improving the quality of software projects and developers support them~\cite{DD08, SS10}. What is more, these practises make software development progress visually and aurally available. The confidence that you are building what the user wants. Teams improve the quality of their work: communication and understanding is improved, knowledge is transferred among the people and developers are more confident about their work. This increases morale and productivity~\cite{SS10}. A productive team is a right mixture of talented people. A team will not work if its members cannot work together. However, it is still clear that many of the practises need more empirical studies to validate their claims~\cite{DD08}.

\subsection{From Agile to Lean}

As time has elapsed, developers have simplified software delivery even more. Agile has turned into Lean. Being “lean” means reducing the amount of “waste” around software development. The principles for Lean development are: eliminating waste, amplifying learning, deciding as late as possible, delivering as fast as possible, empowering the team, building integrity in and seeing the whole~\cite{DD08}.

Iterations have turned into building single features at a time. Instead of building a frame for a car, a development process should essentially start with building a bicycle first. To evaluate an idea, developers should begin by developing a minimum viable product to validate the implementation has value. Even Royce hinted on prototyping and later the spiral model integrated this as a concept~\cite{Roy70, Boe88}. Only a minimal effort should be put into place to specify the overall nature of the product. Being “adaptive” has transformed into quantitatively assessing what effects changes have. This build-measure-learn cycle has transformed how features are developed and validated. See figure~\ref{figure:build-measure-learn}. Either you change you heading by pivoting or you persevere with the choice you have made.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/build-measure-learn}

    \caption{Build-Measure-Learn Cycle}
    \label{figure:build-measure-learn}

\end{figure}

% -- Deployment Pipeline --

\section{Deployment Pipeline}

A deployment pipeline is the basis for many modern development practises. Anything that you can treat as construction should be automated~\cite{Fow05}. This was expensive back in the days, but is not an issue anymore~\cite{Roy70}. One of the obstacles of a automated build and test environment is that you want to be able to build fast so that you can get fast feedback~\cite{Fow13b}. To ensure quality, you have a comprehensive set of tests for your code. Running these tests can take a long time. A deployment pipeline handles this by breaking up your build into multiple stages. Each stage increases trust that everything is working as expected.

The stages can be automated or require human interaction. In fact, they can also be executed in parallel to each other when possible. A deployment pipeline usually begins by building the software. The pipeline runs the automated tests, but can also include manual checks that cannot be automated. Usually deploying software to production is one of the final stages of a deployment pipeline. The purpose of a deployment pipeline is to detect any changes that will lead to issues in production~\cite{Fow13b}. In addition, it gives you visibility about changes in your development process.

\subsection{Development}

Development.

\subsection{Staging}

The idea of a staging environment is to simulate the production environment. Tests should be run under this controlled environment to make sure the software works as intended in the desired environment. By testing on multiple platforms, you ensure the likelihood of the software working also on an other environment. Of course, it is not practical to test you code on every single platform.

\subsection{Production}

Production.

% -- Using Web as a Platform --

\section{Using Web as a Platform}

The acceleration on digital products and services means the web will become more and more irreplaceable for software-intensive products and services. Using Web as a platform: IAAS, PAAS and SAAS.

\subsection{Continuous Integration}

Continuous Integration (CI) is a development practise where members of a team integrate their work frequently, usually multiple times a day~\cite{Fow06}. This leads to multiple integrations of the software every day. Each integration is verified by an automated build and test process to detect any errors as soon as possible. Less time is spent in trying to find bugs, because they are discovered quickly. Only if the source builds and tests without any error, can the overall build be considered good~\cite{Fow06}. If and when a developer breaks the build, it is their responsibility to fix and repeat until the shared state is functional. A feature is only considered done when the CI-process succeeds.

The essence for continuous integrating is maintaining a controlled source code repository~\cite{Fow06}. Software projects involve a lot of files and manually keeping track of these is hard. Source code management allows developers to keep track of changes to the source code and to collaborate with other team members. Everything you need to build the software should be in the repository. Any individual developer works only a few hours at time from this shared project state. After the work is done, the developer integrates their changes back into the repository.

Integration is a way of communicating with the team. Frequent integrations let team members know about changes to the software. This eases any changes necessary in their work. Developers can also see if their work conflicts with an other team member. It also encourages developers the keep their work in as small chunks as possible. This significantly reduces the amount of integration problems by shortening the integration cycle and removing any unpredictability. Conflicts that stay undetected for weeks are hard to resolve~\cite{Fow06}.

The integration process is run locally, but in addition the process should be ran on a separate automated integration machine~\cite{Fow06}. This is generally accomplished with a CI -server. The build can be started manually, but most of the time this process is automated as soon as the developer integrates their work back to the repository. This prevents any flaws that might not be discovered on a local environment. On a CI -server, the build should never stay failed for long.

Continuous Integration assumes a comprehensively automated test suite for the software. The tests are integrated into an integration and build process which in affect results in a stable platform for future development. An integrated system and well-tested software is key for bringing a sense of reality and progress into a project~\cite{Fow05}. Documentation can hide flaws that have not yet been discovered. Untested code can hide even more flaws. Practises such as test-driven development enhance integration by introducing programmers into writing simultaneously tests as they write production code. In addition, writing tests before the implementation is a design practise. Of course, you cannot count tests to find every single bug, but imperfect tests are better than no test at all~\cite{Fow06}. Projects that use CI, tend to have dramatically less bugs~\cite{Fow06}.

Continuous Integration also provides a way of making the latest version of the software being always accessible. Other developers and customers can then demonstrate, explore and see what has changed since the previous version with ease.

\subsection{Continuous Deployment}

Continuous Deployment (CD) is a development practise where you build software throughout its lifecycle so that it can be deployed automatically at any given time~\cite{Fow13a}. CD requires that your pipeline enables you to do Continuous Delivery. The difference between Continuous Delivery and Deployment is that the first enables you to deliver new versions of your software easily with a push of a button whenever you so desire, the latter automates this process by doing deployments automatically to production. This results in many production deployments each day~\cite{OR11, Sny13, Rub14}.

You achieve Continuous Deployment by continuously integrating the features completed by the development team. Teams prioritise keeping software in a deployable state. Features are integrated, built and automatically tested to detect any issues. If no issues are raised, the software can then be deployed automatically to production. Furthermore, you use environments that closely resemble the production environment to first see how the software performs before finally deploying it to users. By making small changes, there is a lower risk of something going wrong. When this happens, it is likely that these issues will be easier to fix.

The value of doing continuous deployments is that the current version of the software can be deployed at a moments notice without panic. Deploying software frequently gives a sense of believable progress, not just developers declaring features done~\cite{Fow13a}. This requires extensive automation throughout the deployment pipeline, but also a close and collaborative working relationship between everyone from developers to system specialists involved in the software delivery~\cite{Fow13a}. Lately this has been referred to as the “DevOps culture”~\cite{Fow13a}. Stakeholders can then test the system and the feedback cycle is short. A substantial risk in the effort of building something is whether or not it is useful to the user. The earlier you have the change of evaluating the value a feature, the quicker you get feedback on it. The web has enabled the possibility to deploy and explore web applications to a subset of users~\cite{Fow06, Fow13a}. This can then be used as factor in making decisions about where to proceed.

\subsection{Continuous Experimentation}

The world is never static, being able to figure out what works and what does not can mean the difference between being in on the top and becoming invisible~\cite{KLS09}. The web has provided a platform for easily establishing a causal relationship between changes and their influence on user-observed behaviour~\cite{KLS09}. In the simplest form of these controlled experimentations, users are randomly assigned to two different variants of a feature: a) the Control and b) the Treatment. The Control represent the existing version of the feature and the Treatment a new version being evaluated. This is also called A/B testing. Data is then collected with predetermined metrics from these experiments — metrics such as how the user behaves with the feature. From these results we can determine by statistical analysis which implementation is better, although not always why. Different implementations can have very unexpected results~\cite{KLS09}. Controlled experiments provide a methodology to reliably evaluate the value of ideas~\cite{KLS09}. By building a system for experimentation, the cost of testing and failure becomes small. This encourages innovation by doing experimentation. Failing fast and knowing when an idea is not great is essential in making course corrections and developing more successful ideas. When we fail fast, we can also make improvements more faster. Due to the distributed nature of the web, these experimentations can be done in the background. New versions of features can be deployed frequently without the user even noticing the changes. This provides an thriving environment for experimentation. Experimentation can be used to understand what the user wants. When an issues is discovered, the feature can be rollbacked promptly, sometimes even automatically.

Continuous Experimentation (CE) is a development practise where you build an environment where you can continuously deploy new features and enhancements to the user~\cite{FGM14}. As a result, developers can continuously get direct feedback from the user by observing usage behaviour. This requires an environment where you automatically deploy new features, collect metrics from usage, analyse them and furthermore integrate the results into the development pipeline. Instead of heavy up-front testing, alerts and post-deployment fixing should be tried~\cite{FGM14}. Continuous Experimentation makes use of minimum viable products as the basis for an hypothesis and experiment. Choices are made by analysing the data gathered from this minimum implementation. The hypothesis is either supported by the data or not. It is necessary to base decisions on sound evidence rather than guesswork~\cite{FGM14}.

% -- Towards Embedded Systems --

\section{Towards Embedded Systems}

Defining fully elaborated requirements work in certain applications where for example security is a important criteria~\cite{Boe88}. It does not however work particularly well with interactive applications that are targeted to end-users. Be that as it may be, even Boehm observed that iterative development suited equally both software and hardware development~\cite{Boe88}.

Predictability may be desired. Organisations such as NASA are prime examples where software development must be predictable. NASA’s operations consist of plenty of procedure, time, large teams and stable requirements~\cite{Fow05}. Having said that, NASA is also a prime example of an organisation where iterative development has been used with good results~\cite{LB03}.

History has many successful examples of the usage of iterative development in software development in embedded systems. The X-15 hypersonic jet applied iterative and incremental development already back in the 1950s~\cite{LB03}. In fact, the X-15 was only a hardware project. In the 1960s this knowledge was carried through to NASA, where iterative development was used in the Project Mercury’s software. The project used surprisingly short iterations that only lasted a half-day. Interestingly, they also applied Extreme Programming practises such as test-driven development. Essentially, the platform for Project Mercury allowed the development team to build the system incrementally. Later in the 1970s, the US Department of Defence used iterative development on large, life-critical space and avionics systems. As an other example, the command and control system for the first US Trident submarine also used iterative development. Although, the project still used very long iterations taking as long as six months each. Other applications of iterative development included TRW/Army Site Defence’s missile defence systems and the US Navy’s Light Airborne Multipurpose System (LAMPS) part of a weapon system. The missile defence software project progressed by the team refining each iteration in response to the preceding iteration’s feedback, an early use of reflection. LAMPS was one of the earliest project that used short iterations that only took one month per iteration. The project succeeded, deliveries were on time and under budget~\cite{LB03}. Another noticeable story is the primary avionics software for NASA’s space shuttle program in the late 1970s. The motivation for using iterative development came from need to be able to handle changing requirements for the shuttle program during its software development. NASA used eight week iterations and these made feedback-driven refinements to specifications. In the early 1990s, a new-generation Canadian Automated Air Traffic Control System was developed using risk-driven iterative development. The project was also a success, despite its near-failure predecessor that applied the famed waterfall model. Still, it used rather long iterations of six months by modern standards. All these examples are early examples of being agile, only applying a fraction of ideas presented by current ideologies.

\subsection{Using Hardware as a Platform}

Using hardware as a platform.

\subsection{Adapting for Deployment Pipeline}

A special test environment may be needed in environments where the implications of feature changes are broad and the customer may have reluctance towards experimenting with new features~\cite{FGM14}. Setting up an experimentation cycle can be rather challenging developing software that requires hardware. Longer release cycles with hardware and potential synchronisation problems between the development schedules is an issue~\cite{FMG14}. Certain life-critical environments, experimentation can be too expensive or undesirable by the stakeholders.

% -- Cases from Embedded Settings --

\section{Cases from Embedded Settings}

Software is considered “soft”, hardware “hard”. It is not always obvious how products or features that combine software with hardware can be developed step-by-step. This combination, usually referred as an embedded system, provides challenges in being agile and adaptive. I conducted several semi-structured interviews with the industry working on leading embedded systems to get a view on if and how the deployment pipeline has changed the development of hardware related products? The interview consisted of the following topics:

\paragraph{Process}

\begin{enumerate}

    \item Do you consider that your organisation follows the principles and practises of Agile and Lean development?
    \item If so, has this recently changed the way you develop products or features into production?
    \item Do you approach development from the point-of-view of the whole product or by single features?
    \item Please describe the process behind developing an idea into a single feature. How long does it take?
    \item Do you recognise distinct development, staging and production environments in your process?
    \item If so, are these automated?

\end{enumerate}

\paragraph{Adapting to Change}

\begin{enumerate}[resume]

    \item How easy or hard is it to adapt changes in hardware related products?
    \item Can you deploy software changes automatically or even remotely?
    \item How do you keep software and hardware development in sync?
    \item How short iterations do you use to adjust for feedback from your stakeholders?

\end{enumerate}

\paragraph{Experimentation}

\begin{enumerate}[resume]

    \item Do you have an automated process for deploying or experimenting a feature?
    \item How do you experiment with software related to hardware?
    \item How do you value an idea (prototypes, minimum-viable products, A/B testing)?
    \item Related to hardware, has new approaches such as electronic testing platforms, 3D-printing or laser-cutting changed your process?

\end{enumerate}

% -- Conclusions --

\section{Conclusions}

Conclusions.

% -- References --

\bibliographystyle{babalpha-lf}
\bibliography{references}

\end{document}
