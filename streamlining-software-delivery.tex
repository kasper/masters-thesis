\documentclass[english]{tktltiki2}

% -- Packages --

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{amsfonts,amsmath,amssymb,amsthm,booktabs,color,enumitem,graphicx}
\usepackage[pdftex,hidelinks]{hyperref}

% Automatically set the PDF metadata fields
\makeatletter
\AtBeginDocument{\hypersetup{pdftitle = {\@title}, pdfauthor = {\@author}}}
\makeatother

% -- Language --

\usepackage[fixlanguage]{babelbib}

% Add bibliography to the table of contents
\usepackage[nottoc]{tocbibind}

% -- tktltiki2 options --

\title{Advances in Streamlining Software Delivery on the Web and its Relations to Embedded Systems}
\author{Kasper Hirvikoski}
\date{\today}
\level{Master’s thesis}
\abstract{Abstract.}

\keywords{keyword}
\classification{}

\begin{document}

% -- Front matter --

\frontmatter

\maketitle
\setcounter{page}{2}
\makeabstract

\tableofcontents

% -- Main matter --

\mainmatter

% -- Introduction --

\section{Introduction}

Software delivery on the web has over the years evolved into a rather established process. A software is developed iteratively through multiple phases, which ensure the user’s requirements and the quality of the product or service. These phases form what is called the deployment pipeline~\cite{Fow06, HF11, Fow13a, Fow13b}.

The deployment pipeline nowadays usually consists of at least three stages: development, staging and production. Organisations alter these depending on their size and needs. Using modern iterative and incremental processes, a software is developed feature-by-feature by iterating through these steps. Development starts in the development stage where developers build the feature requested by the customer or user. The feature is then tested in the staging phase, which represents the production setting. When the feature has been validated, it is then deployed to production. If necessary, each stage can be repeated until the feature is accepted. The steps are short and features are deployed frequently — in some cases even multiple times a day~\cite{OR11, Sny13, Rub14}.

Software engineering consists of various different processes and practises for ensuring the quality of the product or service — nowadays more or less based on Agile and Lean ideologies and practises~\cite{Ono88, BBB01a, Fow05, Mon12}. At the low level, developers use source code management to keep track of changes to the software and to collaborate with other team members. To reinforce that the features work as intended, developers write automated test cases. Teams can also use more social methods — such as reviewing each other’s code — to validate the implementations. These practises form the basis for Continuous Integration and Continuous Deployment~\cite{Fow06, HF11, Fow13a, Fow13b}. Software changes are frequently integrated, tested and deployed — automatically in each stage. The first two form Continuous Integration and the latter Continuous Deployment. If any stage fails, the process starts from the beginning.

The web enables the use of the deployment pipeline and its practises in an unprecedented way~\cite{KLS09}. Due to the distributed nature of the web, software can be deployed as needed and the user always sees the newest version without the need of any interaction. This eases the use of many cutting-edge methods~\cite{KLS09, FGM14}. Deploying software as needed has allowed developers to experiment with different implementations of a feature. These changes can target anything from a more optimised algorithm to something more user-faced, such as improvements to the user experience of a product~\cite{KLS09}. These practises have started to formalise as Continuous Experimentation~\cite{FGM14}.

Not all software can be developed easily this way. Many embedded systems, which have a dedicated function within a larger mechanical or electrical system, require hardware to accompany the software. This presents a variety of challenges to overcome. Hardware can require thorough planning and iterating can take time. Contexts such as cross-platform support, robotics, aerospace and other embedded systems pose interesting cases. Many of these contexts can at a glance seem regarded as models for more traditional sequential software engineering processes with heavy planning, documentation and long development phases. However, even NASA’s earlier missions have iterated on the successes and failures of previous ones~\cite{LB03}. Even though it can be more difficult, software related to hardware can be build and tested iteratively~\cite{LB03}. New approaches such as prototyping and 3D-printing provide novel ways for even building hardware iteratively.

This raises an interesting research topic — \emph{presenting the advances in streamlining software delivery on the web and relating its practises and their advantages and challenges to embedded systems}. Using case studies to identify which Agile and Lean practises are used, how they could be improved and how new practises could be incorporated to these settings. Moreover, the aim is to identify which modern Continuous Integration, Delivery and Experimentation practises are used. Can we determine how they compare to the way the web is utilised as a platform?

The hypothesis is that there should be no reason why these practises could not be successfully used and cleverly adapted to hardware settings. My research method for this thesis was reviewing the current practises in literature and industry. I also conducted several semi-structured interviews with the industry working on leading embedded systems to get a view on if and how the deployment pipeline has changed the development of hardware related products.

This thesis is structured into seven chapters. Following the introduction, Chapter 2 outlines how software delivery has progressed from a structureless process following a code-and-fix mentality to what is now considered the leading edge of iterative development. This sets the scene for understanding the logic behind being adaptive to change and how the user is an essential part of the process. Chapter 3 describes how software delivery has embraced primarily a three staged pipeline for deploying new features to the user. Chapter 4 discusses how the web has provided an effective platform for the deployment pipeline by streamlining and automating many of the practises used by modern development. Chapter 5 delves into the challenges related to delivering software that is firmly linked to hardware. It deliberates about how the deployment pipeline could be integrated into these embedded systems. Chapter 6 presents the results from the interviews I collected from the field. The idea is to incite discussion — through the view of people working on embedded systems — about what is the current state of software delivery in these settings and how it could be improved. Finally Chapter 7 concludes this work by making conclusions about the gathered knowledge.

% -- Software Delivery --

\section{Software Delivery}

Software development has changed notably in the past few decades. Most software development is a disordered chaos with a mentality of coding first and fixing later~\cite{Fow05}. The software is built without much of an underlying plan and the design of the system is a result of many short term decisions. This works well if the system is small, but as it grows adding new features becomes intolerable. Going back, it was not until 1968, when the term software engineering was introduced by the NATO Science Committee~\cite{NR69}. By that time, it was considered that software development had drifted into a crisis, where a wider gap was forming between the objectives and end-results of software projects. Additionally, it was getting increasingly difficult to plan the cost of development. A typical consequence was a long test phase after a system was considered “feature complete”~\cite{Fow05}. A collective effort was put in place to establish a more formalised method for software development — similar to traditional engineering such as building bridges. It was considered necessary that the foundation for delivering software should be more theoretical with laid principles and practises~\cite{NR69}. Software development had to be more predictable and efficient. By 1969, the term software engineering had become well-established~\cite{BR70}.

Software development processes began to form. Notably in 1970, Winston W. Royce published a paper that described a formal approach for sequentially developing a software based on previously used practises~\cite{Roy70}. It was only later named as the waterfall model~\cite{Boe88, LB03}. The process consists of multiple stages that should be carried after the previous has been reviewed and verified. See figure~\ref{figure:waterfall-model}. It begins by mapping the requirements for the entire software, then proceeding to designing the architecture, followed by implementing the plan, verifying the result is according to the set requirements, and finally maintaining the product~\cite{Roy70}. Each stage is planned and documented thoroughly. The concept is that as each step progresses, the design of the software is further detailed. However, contrary to what has been referred, Royce presented the model as a flawed, non-working model~\cite{Roy70}. If any of the stages fail, serious reconsideration of the plan or implementation might be necessary. Therefore sequentially following the stages would not produce what was intended and inevitably previous stages would need to be revisited~\cite{Roy70}. He however found the approach fundamentally sound and proposed that the method should be utilised twice. First to create a prototype-like plan and only then to execute it. Nevertheless, this was overlooked and the waterfall model became the dominant software development process for software standards in government and industry for the time-being~\cite{Boe88, LB03}.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/waterfall-model}

    \caption{Waterfall Model}
    \label{figure:waterfall-model}

    \vspace{1cm}

\end{figure}

Engineering methodologies, also called as plan-driven methods, are considered heavy. The waterfall model has been criticised as too controlled, managed and documentation-oriented~\cite{Boe88, LB03, Fow05}. Royce defined software as done only when the documentation was adequate~\cite{Roy70}. Developers should prioritise in keeping the documentations up to date. They also have not been noted for being terribly successful~\cite{Fow05}. More lightweight iterative processes were proposed as opponents for incremental software development in the later part of the nineteen hundreds~\cite{LB03}. In fact, early applications of iterative and incremental development dates as far back as the mid-1950s~\cite{LB03}. These have had many names such as incremental, evolutionary, spiral and staged~\cite{Fow05}. The methods focused in developing a useful compromise between no process and too much process~\cite{Fow05}. The methods were focused to be less document-oriented and in many ways more code-oriented. The documentation for a project should be the code itself.

Fast-forward to 2001, when a group of software developers met to discuss new lightweight development principles. As the result of these discussions, a manifesto for Agile software development was published~\cite{BBB01a}. Four principles were proposed for Agile software development: \emph{focusing on individuals and interactions} over processes and tools, \emph{focusing on working software} over comprehensive documentation, \emph{focusing on customer collaboration} over contract negotiation and \emph{responding to change} over following a plan. The manifesto does not dismiss the value of the latter, but considers the former more valuable~\cite{BBB01a}. Iterative processes started to gain mainstream traction~\cite{LB03, Fow05}.

Software development was considered as an ongoing process, where a product should be build in small increments, iteratively going through the development stages. Most of the ideas were not now and had been successfully used already in the industry for tens of years~\cite{Fow05}. There was an urge to treat the ideas more seriously. Instead of planning, designing and implementing the whole software, the software should be build iteratively by repeating all of these steps in shorter more manageable parts. See figure~\ref{figure:iterative-development}. The idea was not to resist change. Hence, any issue or miss-communication could be discovered early and fixed accordingly.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/iterative-development}

    \caption{Iterative Development}
    \label{figure:iterative-development}

\end{figure}

\subsection{Adapting to Requirements}

The demands for software products are continuously shifting. It is not always obvious what the users want. In some cases, users do not know what they are looking for, until you show them what they need. It hard to know what the value of a feature is before you see it in reality~\cite{Fow05}. This allow the user to learn how a feature works. An average client has little knowledge on how software products work or how they are built. Therefor, it is exceedingly difficult for a client to map specifically what they require from a software product. Software development should therefor be more people-oriented than process-oriented~\cite{Fow05}. This requires a different kind of relationship with the customer. What is notable is that even Royce emphasised, although loosely, the value of customer commitment during development~\cite{Roy70}.

In most cases, rigorously planning a software beforehand will not work. It is not uncommon that an idea will change quite a bit during its lifetime. The key problem that plan-driven  methods face is the separation of design~\cite{Fow05}. The concept was like in traditional engineering, engineers would build a precise plan which would then be followed by a different set of people. In such, architects would first design the bridge and then a construction company would build it. Designing, which involves creative and more expensive individuals, is far more difficult and less predictable than construction~\cite{Fow05}. Construction on the other hand, although more labour intensive, is considered more predictable and straightforward after a plan has been completed. The premise was that by following this methodology in software engineering, we could reasonably predict the time and cost of software construction. When Royce first defined the waterfall model, he stated that the documentation of a software it both its specification and design~\cite{Roy70}. Without documentation there would be no design, without documentation there would be no communication.

The issue is that no one has found a way of designing software in such a fashion that the plans could be verified before construction~\cite{Fow05}. A design can look good on paper, but be seriously flawed when a you actually program it. When building a bridge, the cost of the design is fractional to the cost of construction~\cite{Fow05}. In software, the time spent coding is fractional to the time spent designing. Essentially, coding is designing. Coding requires creative and talented people. The people are the most important factor in software development. Developers should be in control of all the technical decisions. There are serious flaws in separating different tasks to different “specialists”, but this is how the development was regarded~\cite{Roy70}. It is still quite common that a developer writing the code and a tester writing the tests are not the same person. The metaphor for traditional engineering is in practise flawed~\cite{Fow05}.

Andy Whitlock, a product strategist, drew a fitting mental picture about this~\cite{Whi14}. You see the road ahead as a clear and straight path to an objective you have set. What you do not always realise, is that the path will have its twists and turns along the way. What you can really only do, is to plan to a certain point ahead. The rest of your path will be a gloomy fog in the distance. You need to be ready to make difficult choices along the way. Agile development tries to create a framework, where processes and practises can take these requirements into consideration. Even to the point of changing the process itself~\cite{Fow05}.

Prominently, being “agile” means effectively responding to change. After all software is supposed to be “soft”~\cite{Fow05}. These course corrections are rapid and adaptive. The highest priority is to satisfy the customer trough continuously delivering valuable software from early on~\cite{BBB01b}. Software should be delivered frequently in short increments. These increments, also referred as iterations in Agile development, should take no more than a couple of weeks to a couple of months — the shorter the better~\cite{Fow05}. Each time a working software is delivered with a subset of the required features. These features should as carefully tested as a final delivery. Throughout the project, teams respond to change by having effective communication among all stakeholders for the product daily. The best means for conveying information is face-to-face conversation~\cite{BBB01b}. At every iteration the customer has control over the process by getting a look on the progress and then altering the direction as needed.

A stakeholder represents the views for the users or clients. By taking the stakeholders as part of the team, developers can react when something is not working as intended. Studies show that developers see the ongoing presence of stakeholders helpful for development~\cite{DD08}. An Agile process is driven by the customers descriptions of what is required~\cite{BBB01b}. These requirements may be short-lived and that must be kept in focus. Changes are unavoidable~\cite{Fow05}. Users’ desires evolve and this must be harnessed to the customer’s competitive advantage~\cite{BBB01b, Fow05}. Even if deciding a stable set of requirements would be possible, outside forces are changing the value of features too rapidly~\cite{Fow05}. It is not uncommon for requirements to change even late in development. If you cannot get fixed requirements, you cannot get a predictable plan. This is what makes plan-driven development inefficient. Royce stated that required design changes can be so disruptive that the software requirements upon which the design is based and which provide the rationale for everything can be breached~\cite{Roy70}. Even so predictability is highly desirable~\cite{Fow05}. It is an essential force in what makes models work. Adaptivity is about making unpredictability predictable. Iterative development is about creating risk-control for the project.

One key premiss for Agile development is to reduce the burden of the process. Working software is the primary measure of progress~\cite{BBB01b}. A process should not hinder the work of a team — on the contrary it should permit the team to function to its full extent. By organising the team to be in control of the process, the framework facilitates rapid and incremental delivery of software. No process will make up the skill of the individuals working on the project~\cite{Fow05}. Projects should be based on motivated individuals~\cite{BBB01b}. Motivation is maintained by creating a constructive environment and giving the necessary support when needed. Trusting the team is of the utmost importance~\cite{BBB01b}. Morale affects the productivity of people~\cite{Fow05}.

One of the weaknesses of adaptability is that in its essence it implies that the usual notion of fixed-priced software development does not work~\cite{Fow05}. Instead completely new approaches have to be used. You cannot fix scope, time and price in the same way plan-driven methods have tried. The usual agile approach is to fix time and price and allow the scope to vary in a predetermined manner. The value of is not created by building software on-time and on-cost, but by building software that is valuable to the customer.

\subsection{Ensuring Quality}

Assuring quality is not an easy task. Applying measurements to software development is demanding. Something as simple as productivity is exceedingly hard to quantify. ISO 9000 -standard defines quality as the extent of how well the characteristics of a product or service fulfil all of the requirements, the needs and expectations, set by the stakeholders~\cite{ISO9000}. IEEE defines software quality as the degree to which a system, component or process meets the specified requirements as well as the customer’s and user’s needs or expectations~\cite{IEEE1074}. Both definitions focus strongly on fulfilling the user’s needs.

Software development is challenging. Users perceive quality as working software, but most of all emphasising good technical design and implementation makes the development process easier. People, time and money are limiting factors for ensuring quality. Strict deadlines and scarce resources have direct effects. Furthermore, human factors play a considerable role. Several empirical studies reinforce the significance of Agile development processes and practises as improving quality in software~\cite{SS10}. Evidently being “agile” should in the long term make development more predictable and eventually lead to shorter development times and minimised costs. This provides an environment for being adaptive.

In addition to focusing on satisfying the customers needs, Agile development promotes continuous attention on technical excellence and good design practises~\cite{BBB01b}. Even so, this should not be accomplished by hindering simplicity. Simplicity maximises the amount of work that can be accomplished. The Agile Manifesto states that the best architectures, requirements and designs emerge from self-organising teams~\cite{BBB01b}. After regular intervals, the team members reflect on how they have performed and how they can become more effective. This is how the team can then tune and adjust its behaviour appropriately.

\subsection{Processes and Practises}

Processes and practises assist the development process. They create the framework and guidelines within a team can develop a suitable environment to deliver software~\cite{Kni07}. The process is part of the design~\cite{Fow05}. They also help to maintain quality.

At the low level, developers use source code management to keep track of changes to the software and to collaborate with other team members. Source code management enables multiple developers to work on a single project, while also creating a history for the entire project. When a problem arises, developers can go back in time to look at the source code at any given point in time. To ensure features work as intended, developers use automated test cases to verify expected behaviour. There is a clear correlation between higher test coverage resulting in fewer errors in software~\cite{MND09}. Tested code has a better change of detecting errors than untested code. Teams can also use more social methods — such as reviewing each other’s code — to validate the implementations. Pair programming, coding dojos and hackathons provide tools for improving skills and solving complex problems together~\cite{HHL13}.

Agile development only provides a framework for software delivery. It does not specify concretely how development should be organised. Most notably, Scrum and Extreme Programming (XP) have created a structure for Agile development~\cite{LB03, Fow05, SS10}. Scrum provides a framework for managing development. It focuses on how development should be planned, managed and scheduled. It does not provide any strict practises, instead it gives guidelines for how customer requirements should be discovered, prioritised, and how the development of these features is split into iterations.

Scrum has been strengthened with practises such as Continuous Integration, test-driven development (TDD) and pair programming. These are defined in XP. In test-driven development, features are developed by writing the expectations for a feature as tests before actually implementing the code. In pair programming, developers develop features in pairs.

\subsection{From Agile to Lean}

As time has elapsed, developers have simplified software delivery even more. Agile has turned into Lean. Being “lean” means reducing the amount of “waste” around software development. Iterations have turned into building single features at a time. Instead of building a frame for a car, a development process should essentially start with building a bicycle first. To evaluate an idea, developers should begin by developing a minimum viable product (MVP) to validate the implementation has value. Being “adaptive”, has transformed into quantitatively assessing what effects changes have. This build-measure-learn cycle has transformed how features are developed and validated. See figure~\ref{figure:build-measure-learn}.

\begin{figure}[h!]

    \vspace{1cm}
    \centering

    \includegraphics{figures/build-measure-learn}

    \caption{Build-Measure-Learn Cycle}
    \label{figure:build-measure-learn}

\end{figure}

% -- Deployment Pipeline --

\section{Deployment Pipeline}

A deployment pipeline is the basis for many modern development practises. Anything that you can treat as construction should be automated~\cite{Fow05}. This was expensive back in the days~\cite{Roy70}. One of the obstacles of a automated build and test environment is that you want to be able to build fast so that you can get fast feedback~\cite{Fow13b}. To ensure quality, you have a comprehensive set of tests for your code. Running these tests can take a long time. A deployment pipeline handles this by breaking up your build into multiple stages. Each stage increases trust that everything is working as expected.

A deployment pipeline usually begins by building the software. The stages can be automated or require human interaction. In fact, they can also be executed in parallel to each other when possible. The pipeline runs the automated test, but can also include manual check that cannot be automated. Usually deploying the software to production is one of the final stages of a deployment pipeline. The purpose of a deployment pipeline is to detect any changes that will lead to issues in production~\cite{Fow13b}. In addition, it gives you visibility about changes in your development process.

\subsection{Development}

Development.

\subsection{Staging}

The idea of a staging environment is to simulate the production environment. Test should be run under these controlled environment to make sure the software works as intended in the correct environment. By testing on multiple platform, you ensure the likelihood of the software working on an other environment. Of course it is not practical to test you code on every single platform.

\subsection{Production}

Production.

% -- Using Web as a Platform --

\section{Using Web as a Platform}

Using Web as a platform. IAAS, PAAS and SAAS.

\subsection{Continuous Integration}

Continuous Integration (CI) is a development practise where members of a team integrate their work frequently, usually multiple times a day~\cite{Fow06}. This leads to multiple integrations of the software every day. Each integration is verified by an automated build and test process to detect any errors as soon as possible. Less time is spent in trying to find bugs, because they are discovered quickly. Only if the source builds and tests without any error, can the overall build be considered good~\cite{Fow06}. If and when a developer breaks the build, it is their responsibility to fix and repeat until the shared state is functional. A feature is only considered done when the CI succeeds.

The essence for CI is maintaining a controlled source code repository~\cite{Fow06}. Software projects involve a lot of files and manually keeping track of these is burdensome. Source code management allows developers to keep track of changes to the source code for the software and to collaborate with other team members. Everything you need to build the software should be in the repository. Any individual developer works only a few hours at time from this shared project state. After the work is done, the developer integrates their changes back into the repository.

Integration is a way of communicating with the team. Frequent integrations let team members know about changes to the software. This eases any changes necessary in their work. Developers can also see if their work conflicts with an other team member. It also encourages developers the keep their work in as small chunks as possible. This significantly reduces the amount of integration problems by shortening the integration cycle and removing any unpredictability. Conflicts that stay undetected for weeks are hard to resolve~\cite{Fow06}.

The integration process is run locally, but in addition the process should be ran on a separate automated integration machine~\cite{Fow06}. This is generally accomplished with a CI server. The build can be started manually, but most of the time this process is automated. A build is started automatically as soon as the developer integrates their work to the repository. This prevents any flaws that might not be discovered on a local environment. In this environment, the build should never stay failed for long.

CI assumes a comprehensively automated test suite for the software. The tests are integrated into an integration and build process which in affect results in a stable platform for future development. An integrated system and well-tested software is key for bringing a sense of reality and progress into a project~\cite{Fow05}. Documentation can hide flaws that have not yet been discovered. Untested code can hide even more of flaws. Practises such as test-driven development enhance integration by introducing programmers into writing simultaneously tests as they write production code. In addition, writing tests before the implementation is a design practise. Of course, you cannot count test to find every single bug, but imperfect tests are better than no test at all~\cite{Fow06}. Project that use CI, tend to have dramatically less bugs~\cite{Fow06}.

CI also provides a way of making the latest version of the software being always accessible. Other developers and customers can then demonstrate, explore and see what has changed since the previous version.

\subsection{Continuous Deployment}

Continuous Deployment (CD) is a development practise where you build software throughout its lifecycle so that it can be deployed automatically at any given time~\cite{Fow13a}. CD requires that your pipeline enables you to do Continuous Delivery. The difference between Continuous Delivery and Deployment is that the first enables you to deliver new versions of your software easily with a push of a button whenever you so desire, the latter automates this process by doing deployments automatically to production. This results in many production deployments each day~\cite{OR11, Sny13, Rub14}.

You achieve CD by continuously integrating the features completed by the development team. Teams prioritise keeping software in a deployable state. Features are integrated, built and automatically tested to detect any issues. If no issues are raised, the software can then be deployed automatically to production. Furthermore, you use environments that closely resemble the production environment to first see how the software performs before finally deploying it to users. By making small changes, there is a lower risk of something going wrong. When this, happens it likely that these issues will be easier to fix.

The value of doing continuous deployments is that the current version of the software can be deployed at a moments notice without panic. Deploying software frequently gives a sense of believable progress, not just developers declaring features done~\cite{Fow13a}. This requires extensive automation on through the deployment pipeline, but also a close and collaborative working relationship between everyone from developers to system specialists involved in the software delivery~\cite{Fow13a}. Lately this has been referred to as a “DevOps culture”~\cite{Fow13a}. The stakeholders can then test the system and the feedback cycle is short. A substantial risk in the effort of building something is whether or not is useful to the user. The earlier you have the change of evaluating the value a feature, the quicker you get feedback on it. The web has enabled to possibility to deploy and explore web applications to a subset of users~\cite{Fow06, Fow13a}. This can then be used as factor in making decisions about where to proceed.

\subsection{Continuous Experimentation}

Continuous Experimentation.

% -- Towards Embedded Systems --

\section{Towards Embedded Systems}

Predictability may be desired. Organisations such as NASA are prime examples where software development must be predictable. NASA’s operations consist of plenty of procedure, time, large teams and stable requirements~\cite{Fow05}. Having said that NASA is also a prime example of an organisation where iterative development has been used with good results~\cite{LB03}.

\subsection{Using Hardware as a Platform}

Using hardware as a platform.

\subsection{Adapting for Deployment Pipeline}

Adapting for deployment pipeline.

% -- Cases from Embedded Settings --

\section{Cases from Embedded Settings}

Software is considered “soft”, hardware “hard”. It is not always obvious how products or features that combine software with hardware can be developed step-by-step. This combination, usually referred as an embedded system, provides challenges in being agile and adaptive. I conducted several semi-structured interviews with the industry working on leading embedded systems to get a view on if and how the deployment pipeline has changed the development of hardware related products? The interview consisted of the following topics:

\paragraph{Process}

\begin{enumerate}

    \item Do you consider that your organisation follows the principles and practises of Agile and Lean development?
    \item If so, has this recently changed the way you develop products or features into production?
    \item Do you approach development from the point-of-view of the whole product or by single features?
    \item Please describe the process behind developing an idea into a single feature. How long does it take?
    \item Do you recognise distinct development, staging and production environments in your process?
    \item If so, are these automated?

\end{enumerate}

\paragraph{Adapting to Change}

\begin{enumerate}[resume]

    \item How easy or hard is it to adapt changes in hardware related products?
    \item Can you deploy software changes automatically or even remotely?
    \item How do you keep software and hardware development in sync?
    \item How short iterations do you use to adjust for feedback from your stakeholders?

\end{enumerate}

\paragraph{Experimentation}

\begin{enumerate}[resume]

    \item Do you have an automated process for deploying or experimenting a feature?
    \item How do you experiment with software related to hardware?
    \item How do you value an idea (prototypes, minimum-viable products, A/B testing)?
    \item Related to hardware, has new approaches such as electronic testing platforms, 3D-printing or laser-cutting changed your process?

\end{enumerate}

% -- Conclusions --

\section{Conclusions}

Conclusions.

% -- References --

\bibliographystyle{babalpha-lf}
\bibliography{references}

\end{document}
